{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Problem_Statement_Dependency_Deep Q Learning Stock Trading_CEP_2.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fgy74k3DDaf_","colab_type":"text"},"source":["#**Stock Trading Using Deep Q-Learning**\n"]},{"cell_type":"markdown","metadata":{"id":"LW3MPB3pDndR","colab_type":"text"},"source":["## **Problem Statement**"]},{"cell_type":"markdown","metadata":{"id":"FeOMTczqDqdZ","colab_type":"text"},"source":["Prepare an agent by implementing Deep Q-Learning that can perform unsupervised trading in stock trade. The aim of this project is to train an agent that uses Q-learning and neural networks to predict the profit or loss by building a model and implementing it on a dataset that is available for evaluation.\n","\n","\n","The stock trading index environment provides the agent with a set of actions:<br>\n","* Buy<br>\n","* Sell<br>\n","* Sit\n","\n","This project has following sections:\n","* Import libraries \n","* Create a DQN agent\n","* Preprocess the data\n","* Train and build the model\n","* Evaluate the model and agent\n","<br><br>\n","\n","**Steps to perform**<br>\n","\n","In the section **create a DQN agent**, create a class called agent where:\n","* Action size is defined as 3\n","* Experience replay memory to deque is 1000\n","* Empty list for stocks that has already been bought\n","* The agent must possess the following hyperparameters:<br>\n","  * gamma= 0.95<br>\n","  * epsilon = 1.0<br>\n","  * epsilon_final = 0.01<br>\n","  * epsilon_decay = 0.995<br>\n","\n","\n","    Note: It is advised to compare the results using different values in hyperparameters.\n","\n","* Neural network has 3 hidden layers\n","* Action and experience replay are defined\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Lu4reAtsL5EZ","colab_type":"text"},"source":["## **Solution**"]},{"cell_type":"markdown","metadata":{"id":"WBgbvVTRDXpe","colab_type":"text"},"source":["### **Import the libraries** "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zt5QkvOCri3W","colab":{}},"source":["import keras\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","import numpy as np\n","import random\n","from collections import deque"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"behdrRbIDXpj","colab_type":"text"},"source":["### **Create a DQN agent**"]},{"cell_type":"markdown","metadata":{"id":"vrJH6vRmNZQw","colab_type":"text"},"source":["**Use the instruction below to prepare an agent**\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"w7uHLPJWtNmm","colab":{}},"source":["# Action space include 3 actions: Buy, Sell, and Sit\n","#Setting up the experience replay memory to deque with 1000 elements inside it\n","#Empty list with inventory is created that contains the stocks that were already bought\n","#Setting up gamma to 0.95, that helps to maximize the current reward over the long-term\n","#Epsilon parameter determines whether to use a random action or to use the model for the action. \n","#In the beginning random actions are encouraged, hence epsilon is set up to 1.0 when the model is not trained.\n","#And over time the epsilon is reduced to 0.01 in order to decrease the random actions and use the trained model\n","#We're then set the speed of decreasing epsililon in the epsilon_decay parameter\n","\n","#Defining our neural network:\n","#Define the neural network function called _model and it just takes the keyword self\n","#Define the model with Sequential()\n","#Define states i.e. the previous n days and stock prices of the days\n","#Defining 3 hidden layers in this network\n","#Changing the activation function to relu because mean-squared error is used for the loss\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-nRItH8mDXpm","colab_type":"text"},"source":["### **Preprocess the stock market data**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e3NGhJubtfet","colab":{}},"source":["import math\n","\n","# prints formatted price\n","def formatPrice(n):\n","\treturn (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n","\n","# returns the vector containing stock data from a fixed file\n","def getStockDataVec(key):\n","\tvec = []\n","\tlines = open(\"\" + key + \".csv\", \"r\").read().splitlines()\n","\n","\tfor line in lines[1:]:\n","\t\tvec.append(float(line.split(\",\")[4]))\n","\n","\treturn vec\n","\n","# returns the sigmoid\n","def sigmoid(x):\n","\treturn 1 / (1 + math.exp(-x))\n","\n","# returns an an n-day state representation ending at time t\n","def getState(data, t, n):\n","\td = t - n + 1\n","\tblock = data[d:t + 1] if d >= 0 else -d * [data[0]] + data[0:t + 1] # pad with t0\n","\tres = []\n","\tfor i in range(n - 1):\n","\t\tres.append(sigmoid(block[i + 1] - block[i]))\n","\n","\treturn np.array([res])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b9oHyPgfDXpp","colab_type":"text"},"source":["### **Train and build the model**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fCqJzkeJtp3n","colab":{}},"source":["import sys\n","\n","if len(sys.argv) != 4:\n","\tprint (\"Usage: python train.py [stock] [window] [episodes]\")\n","\texit()\n","\n","\n","stock_name = input(\"Enter stock_name, window_size, Episode_count\")\n","#Fill the given information when prompted: \n","#Enter stock_name = GSPC_Training_Dataset\n","#window_size = 10\n","#Episode_count = 100 or it can be 10 or 20 or 30 and so on.\n","\n","window_size = input()\n","episode_count = input()\n","stock_name = str(stock_name)\n","window_size = int(window_size)\n","episode_count = int(episode_count)\n","\n","agent = Agent(window_size)\n","data = getStockDataVec(stock_name)\n","l = len(data) - 1\n","batch_size = 32\n","\n","for e in range(episode_count + 1):\n","\tprint (\"Episode \" + str(e) + \"/\" + str(episode_count))\n","\tstate = getState(data, 0, window_size + 1)\n","\n","\ttotal_profit = 0\n","\tagent.inventory = []\n","\n","\tfor t in range(l):\n","\t\taction = agent.act(state)\n","\n","\t\t# sit\n","\t\tnext_state = getState(data, t + 1, window_size + 1)\n","\t\treward = 0\n","\n","\t\tif action == 1: # buy\n","\t\t\tagent.inventory.append(data[t])\n","\t\t\tprint (\"Buy: \" + formatPrice(data[t]))\n","\n","\t\telif action == 2 and len(agent.inventory) > 0: # sell\n","\t\t\tbought_price = agent.inventory.pop(0)\n","\t\t\treward = max(data[t] - bought_price, 0)\n","\t\t\ttotal_profit += data[t] - bought_price\n","\t\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n","\n","\t\tdone = True if t == l - 1 else False\n","\t\tagent.memory.append((state, action, reward, next_state, done))\n","\t\tstate = next_state\n","\n","\t\tif done:\n","\t\t\tprint (\"--------------------------------\")\n","\t\t\tprint (\"Total Profit: \" + formatPrice(total_profit))\n","\t\t\t\n","\n","\t\tif len(agent.memory) > batch_size:\n","\t\t\tagent.expReplay(batch_size)\n","\n","\t#if e % 10 == 0:\n","\t\tagent.model.save(\"model_ep\" + str(e))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UcXCrJUSDXpr","colab_type":"text"},"source":["### **Evaluate the model and agent**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lmZUVXe5t95k","colab":{}},"source":["import sys\n","from keras.models import load_model\n","\n","\n","if len(sys.argv) != 3:\n","\tprint (\"Usage: python evaluate.py [stock] [model]\")\n","\texit()\n","\n","\n","stock_name = input(\"Enter Stock_name, Model_name\")\n","model_name = input()\n","#Note: \n","#Fill the given information when prompted: \n","#Enter stock_name = GSPC_Evaluation_Dataset\n","#Model_name = respective model name\n","\n","model = load_model(\"\" + model_name)\n","window_size = model.layers[0].input.shape.as_list()[1]\n","\n","agent = Agent(window_size, True, model_name)\n","data = getStockDataVec(stock_name)\n","l = len(data) - 1\n","batch_size = 32\n","\n","state = getState(data, 0, window_size + 1)\n","total_profit = 0\n","agent.inventory = []\n","\n","for t in range(l):\n","\taction = agent.act(state)\n","\n","\t# sit\n","\tnext_state = getState(data, t + 1, window_size + 1)\n","\treward = 0\n","\n","\tif action == 1: # buy\n","\t\tagent.inventory.append(data[t])\n","\t\tprint (\"Buy: \" + formatPrice(data[t]))\n","\n","\telif action == 2 and len(agent.inventory) > 0: # sell\n","\t\tbought_price = agent.inventory.pop(0)\n","\t\treward = max(data[t] - bought_price, 0)\n","\t\ttotal_profit += data[t] - bought_price\n","\t\tprint (\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n","\n","\tdone = True if t == l - 1 else False\n","\tagent.memory.append((state, action, reward, next_state, done))\n","\tstate = next_state\n","\n","\tif done:\n","\t\tprint (\"--------------------------------\")\n","\t\tprint (stock_name + \" Total Profit: \" + formatPrice(total_profit))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIolgCRWSM-9","colab_type":"text"},"source":["**Note: Run the training section for considerable episodes so that while evaluating the model it can generate significant profit.** \n"]}]}